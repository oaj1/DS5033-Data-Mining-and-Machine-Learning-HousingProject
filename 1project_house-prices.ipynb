{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project One: House Prices\n",
    "\n",
    "Oliver James\n",
    "DS 5033 Data Mining & Machine Learning\n",
    "\n",
    "During the course of this project, you will learn to load, process, visualize and predict with housing data.\n",
    "We will work through a series of steps:\n",
    "1. Understand the Overarching Task\n",
    "2. Acquire and Load the Data\n",
    "3. Prepare the Data (for ML)\n",
    "4. Selecting and Training an ML Model\n",
    "5. Presenting an Analysis on the Solution\n",
    "\n",
    "We will consolidate this notebook into three following steps:\n",
    "\n",
    "* Broadly speaking, tasks 1-3 fall under Exploratory Data Analysis (EDA).\n",
    "* Then, task four encompulates several steps for price price prediction (the modeling side of things).\n",
    "* Finally, we have our analysis in task five.\n",
    "\n",
    "\n",
    "Due to some difficulties in acquiring a dataset on Texas and San Antonio housing, we will be relying on an\n",
    "older dataset. For the purposes of learning, the data is still relevant.\n",
    "\n",
    "For this project, the data is on Californian houses (when they were still affordable).  \n",
    "The csv is also attached to the assignment.\n",
    "\n",
    "* The dataset is available from: https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
    "* You can also get the csv from: https://github.com/ageron/data/blob/main/housing/housing.csv\n",
    "* Read more here: https://www.kaggle.com/datasets/camnugent/california-housing-prices/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T19:11:37.818457Z",
     "iopub.status.busy": "2024-11-04T19:11:37.818032Z",
     "iopub.status.idle": "2024-11-04T19:11:37.826055Z",
     "shell.execute_reply": "2024-11-04T19:11:37.824859Z",
     "shell.execute_reply.started": "2024-11-04T19:11:37.818407Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#install any libraries you may need\n",
    "import pandas as pd # for dataframe creation\n",
    "from pandas.plotting import scatter_matrix #to create scatter matrix for initial EDA\n",
    "import matplotlib.pyplot as plt #for plotting data\n",
    "import seaborn as sns\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler #This will be used to scale data t\n",
    "from sklearn.compose import ColumnTransformer #To allow the application of different transformations (e.g., scaling, encoding) to different columns of your dataset in one unified step.\n",
    "from sklearn.pipeline import Pipeline #To combine preprocessing and modeling steps into one object, making your code cleaner and reducing the chances of data leakage by ensuring preprocessing is applied consistently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (40%)\n",
    "\n",
    "The task here is to construct a big picture view of the data set.\n",
    "\n",
    "1. Download and load the dataset using pandas\n",
    "2. Print statistics - use the head, info, describe functions\n",
    "3. Visualize the data using the scatter_matrix function \n",
    "4. Visualize the data according to geospatial data (HINT: plot using coordinate data)\n",
    "5. Improve your geospatial plot: change parameters alpha, s, c, cmap. (your end result should have a colorbar with price and datapoints showing housing density with the hues indicating the price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#EDA\n",
    "# Step 1: Load the dataset\n",
    "df = pd.read_csv('housing.csv')  # read CSV file\n",
    "\n",
    "# Step 2: Print statistics\n",
    "print(\"First 5 rows of the data:\")\n",
    "print(df.head())  # get initial look at data\n",
    "\n",
    "#Please see next code block for step 3, it is a rather large graph, so I've given it it's own code block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA Continued\n",
    "#Step 3\n",
    "# Only include relevant numeric columns to avoid clutter\n",
    "scatter_matrix(df[['housing_median_age', 'total_rooms', 'total_bedrooms', 'population', \n",
    "                   'households', 'median_income', 'median_house_value']], \n",
    "               figsize=(20, 10), alpha=0.3, diagonal='kde')\n",
    "plt.suptitle(\"Scatter Matrix of Housing Data\", size=16)\n",
    "plt.show()\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA Continued\n",
    "# Steps 4 and 5 are here for EDA\n",
    "# Select only the numeric columns from your DataFrame\n",
    "numeric_df = df.select_dtypes(include=['float64', 'int64'])\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.text(0, -0.5, \"For above Matrix: Correlation values range from -1 (negative correlation) to +1 (positive correlation).\\nValues close to 0 indicate little or no linear relationship.\",\n",
    "         ha='center', va='top', fontsize=12, transform=plt.gca().transAxes)\n",
    "plt.show()\n",
    "\n",
    "# Step 4 & 5 combined: Improved Geospatial Plot with adjustments to alpha, s, c, and cmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(df['longitude'], df['latitude'], \n",
    "            c=df['median_house_value'],  # Color represents median house value\n",
    "            cmap='coolwarm',             # Color map for better contrast\n",
    "            s=df['population'] / 50,     # Adjust size to represent population\n",
    "            alpha=0.4)                   # Transparency for overlapping points\n",
    "plt.colorbar(label='Median House Value')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Improved Geospatial Plot of Housing Prices and Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep and Cleaning (20%)\n",
    "\n",
    "Here, you will work to further investigate your features.\n",
    "\n",
    "1. Check your features, are they in the correct units (median_income)\n",
    "2. Find and fix missing values\n",
    "3. Handle categorical data (HINT: OneHotEncoder)\n",
    "4. Check the scaling for data. Does it need adjustment? (HINT: StandardScaler vs MinMaxScaler)\n",
    "5. EXTRA CREDIT: how are you fixing (2)? Use SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Data Prep and Cleaning\n",
    "\n",
    "#use multiple code blocks to organize your code\n",
    "#Step 1, fix median income to be in the 10ks... So multiply everything by 10,000\n",
    "\n",
    "df_1 = pd.read_csv('housing.csv')\n",
    "df_1['median_income'] = df_1['median_income'] *10000\n",
    "df_1.drop_duplicates()\n",
    "df_1.dropna()\n",
    "df_1.to_csv('housing_updated1.csv', index=False)\n",
    "\n",
    "\n",
    "#step 2, find and fix missing values\n",
    "\n",
    "# Check for missing values in each column\n",
    "missing_values = df_1.isnull().sum()\n",
    "print(\"Missing values in each column:\")\n",
    "print(missing_values)\n",
    "print()\n",
    "\n",
    "\n",
    "#we see that  there 207 missing values in total bedrooms column\n",
    "print('207 missing values for total bedrooms field, to deal with this issue I will fill it with the median total bedrooms\\n')\n",
    "df_1['total_bedrooms'] = df_1['total_bedrooms'].fillna(df['total_bedrooms'].median())\n",
    "\n",
    "# Check if the missing values are filled\n",
    "missing_values = df_1.isnull().sum()\n",
    "print(\"Missing values after filling:\")\n",
    "print(missing_values) #solved the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Prep and cleaning\n",
    "#Step 3 handle categorical data, OneHotEncoder \n",
    "\n",
    "print('Now I am going to take a look at the field Ocean_Promixity, since that is the only field that is categorical\\n')\n",
    "\n",
    "# Get unique values\n",
    "unique_values = df_1['ocean_proximity'].unique()\n",
    "\n",
    "# Print the unique values\n",
    "print(f\"Unique values in the column: \\n\")\n",
    "print(unique_values)\n",
    "\n",
    "#now I need to encode these values respecively, and update my CSV file\n",
    "# Perform one-hot encoding\n",
    "one_hot_encoded = pd.get_dummies(df_1['ocean_proximity'], prefix='ocean_proximity')\n",
    "\n",
    "# Convert encoded columns to integers to ensure they appear as 1 and 0\n",
    "one_hot_encoded = one_hot_encoded.astype(int)\n",
    "\n",
    "# Concatenate the original dataframe with the encoded columns\n",
    "df_updated = pd.concat([df_1, one_hot_encoded], axis=1)\n",
    "\n",
    "# Drop the original 'ocean_proximity' column\n",
    "df_updated.drop(columns=['ocean_proximity'], inplace=True)\n",
    "\n",
    "# Save the updated dataframe back to a CSV file\n",
    "df_updated.to_csv('housing_updated1.csv', index=False)\n",
    "\n",
    "print(\"\\nThe one-hot encoding is complete, and the updated CSV file has been saved as 'housing_updated1.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Prep and Cleaning continued\n",
    "#Step 4, check scaling of data. \n",
    "#I used ChatGPT to look at my updated excel file and look at which fields need adjusting, see below recommendations\n",
    "#It created scatter plots, and I looked for normal, or near normal distribution for standard and skewed for minMaxScaler\n",
    "\"\"\"\"\"\n",
    "Key Considerations for Scaling:\n",
    "StandardScaler: Works well for normally distributed data, scales to zero mean and unit variance. Suitable for variables like Median Income and Median House Value.\n",
    "MinMaxScaler: Rescales data to a fixed range, typically [0, 1]. Better for preserving data structure in features with skewed distributions like Total Rooms and Population.\n",
    "Recommendation:\n",
    "Use StandardScaler for features like Median Income and Median House Value.\n",
    "Use MinMaxScaler for skewed data like Total Rooms, Total Bedrooms, Population, and Households.\n",
    "\"\"\"\n",
    "\n",
    "# Define transformations\n",
    "preprocessing = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('std', StandardScaler(), ['housing_median_age', 'median_income']),\n",
    "        ('minmax', MinMaxScaler(), ['total_rooms', 'total_bedrooms', 'population', 'households'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit and transform the dataset\n",
    "scaled_data = preprocessing.fit_transform(df_updated)\n",
    "\n",
    "# Convert to DataFrame\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=['housing_median_age', 'median_income', 'total_rooms', 'total_bedrooms', 'population', 'households'])\n",
    "print(scaled_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price Prediction (20%)\n",
    "\n",
    "Now you will create and test many ML models. The idea is to play with hyperparameters and model types.\n",
    "You may find that some of your data prep needs further tweaking.\n",
    "\n",
    "Now that you understand the big picture and have an intuition on how homes may be priced, you can move on to creating a price model.\n",
    "Generally, you will want to follow the order of steps below (with 1&2 already completed):\n",
    "\n",
    "1. Create a copy of your data.\n",
    "2. On the copy, perform the data prep transformations.\n",
    "3. Create a train & test split. Generally, 80-20 splits work pretty well.\n",
    "4. Use a model you understand well. We recently learned how LinearRegression. How well does this model work?\n",
    "5. In our introductory class, we also learned about the DecisionTreeRegressor. How does this perform?\n",
    "6. EXTRA CREDIT: Model prices with the RandomForestRegressor model\n",
    "\n",
    "The main idea here is to get used to programming and modeling on real data. Report your results and aim for\n",
    "a ballpark Root Mean Squared Error less than 75K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#use multiple code blocks to organize your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis (20%):\n",
    "\n",
    "A critical component in science is communicating your results and explaining the reseasoning behind the results.\n",
    "A good presentation here should include the following:  \n",
    "\n",
    "1. An introduction to the dataset, any things we should know (e.g. how it was collected, common errors). \n",
    "2. What did you discover in your EDA? What do you do with missing values, outliers, etc.  \n",
    "3. What kind of distribution is the data? Is there a skew or high concentration of houses in a certain range?  \n",
    "4. What correlations were revealed in the analysis? e.g. footage and price, do they correlate positively?  \n",
    "5. Feature selection: which feature worked for price predictions and what was noise? How did you determine?\n",
    "\n",
    "The points above are for guidance; you can choose your template and structure.  \n",
    "The idea is to present a short report (no word counts) that is structured. \n",
    "structured, clear, and concise.  \n",
    "You can refer back to your figures and use external links to explain your insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use this markdown cell to write a report\n",
    "\n",
    "[here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission:\n",
    "\n",
    "You need to prepare your ipynb/jupyter notebook for grading.\n",
    "The two main tasks are ensuring all your cell outputs are present and that you convert the notebook to PDF.\n",
    "\n",
    "The instructs will vary slightly based on the platform (collab, kaggle, anaconda, etc).\n",
    "Generally, inside the notebook, you will want to:\n",
    "1. Restart & clear all cell outputs (optional, may detect buggy program control flow)\n",
    "2. Run all (must do; I need to see your code cell outputs!)\n",
    "\n",
    "Next, you need to download the notebook as a PDF. Unfortunately, exporting as PDF is a bit tricky.\n",
    "An easy work around:\n",
    "1. Download the notebook. (all platforms allow the default .ipynb export)\n",
    "2. https://onlineconvertfree.com/convert-format/ipynb-to-pdf/\n",
    "\n",
    "If you are unable to upload as a PDF, submit the .ipynb. Do NOT upload a .py file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rubric:\n",
    "\n",
    "Please see the associated percentage allocations.  \n",
    "In general, ensure your code runs correctly.  Make your the PDF upload includes your code ouputs.  \n",
    "You will be given significant credit for documentation and pseudo-code.\n",
    "\n",
    "For more details, please read the rubric PDF in the assignment files."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
